{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Introduction to frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?\n",
    "\n",
    "- Spark is a framework for distributed processing.\n",
    "- It is a streamlined alternative to Map-Reduce.\n",
    "- Spark applications can be written in Scala, Java, or Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark?\n",
    "\n",
    "Why learn Spark?\n",
    "\n",
    "- Spark enables you to analyze petabytes of data.\n",
    "- Spark is significantly faster than Map-Reduce.\n",
    "- Paradoxically, Spark's API is simpler than the Map-Reduce API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origins\n",
    "\n",
    "- Spark was initially started at UC Berkeley's AMPLab (AMP = Algorithms Machines People) in 2009.\n",
    "- After being open sourced in 2010 under a BSD license, the project was donated in 2013 to the Apache Software Foundation and switched its license to Apache 2.0.\n",
    "- Spark is one of the most active projects in the Apache Software Foundation and one of the most active open source big data projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essense of Spark\n",
    "\n",
    "What is the basic idea of Spark?\n",
    "- Spark takes the Map-Reduce paradigm and changes it in some critical ways:\n",
    "  - Instead of writing single Map-Reduce jobs, a Spark job consists of a series of map and reduce functions.\n",
    "  - Moreover, the intermediate data is kept in memory instead of being written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Ecosystem\n",
    "\n",
    "<img src='assets/spark_ecosystem.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Since Spark keeps intermediate data in memory to get speed, what does it make us give up? Where's the catch?</summary>\n",
    "1. Spark does a trade-off between memory and performance.\n",
    "<br>\n",
    "2. While Spark jobs are faster, they also consume more memory.\n",
    "<br>\n",
    "3. Spark outshines Map-Reduce in iterative algorithms where the overhead of saving the results of each step to HDFS slows down Map-Reduce.\n",
    "<br>\n",
    "4. For non-iterative algorithms Spark is comparable to Map-Reduce.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Logging\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "- By default Spark logs messages at the `INFO` level.\n",
    "- Here are the steps to make it only print out warnings and errors.\n",
    "\n",
    "```sh\n",
    "cd $SPARK_HOME/conf\n",
    "cp log4j.properties.template log4j.properties\n",
    "```\n",
    "\n",
    "- Edit `log4j.properties` and replace `rootCategory=INFO` with `rootCategory=ERROR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Execution\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "## Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term | Meaning\n",
    "--- |---\n",
    "Driver | Process that contains the Spark Context\n",
    "Executor | Process that executes one or more Spark tasks\n",
    "Master | Process which manages applications across the cluster, e.g., Spark Master\n",
    "Worker | Process which manages executors on a particular worker node, e.g. Spark Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Job\n",
    "\n",
    "Q: Flip a coin 100 times using Python's `random()` function. What\n",
    "fraction of the time do you get heads?\n",
    "\n",
    "- Initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .master('local[4]') \\\n",
    "    .appName('spark-lecture') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define and run the Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads = 500\n",
      "tails = 500\n",
      "ratio = 0.5\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "n = 1000\n",
    "\n",
    "\n",
    "heads = (sc.parallelize(range(n))\n",
    "    .map(lambda _: random.random())\n",
    "    .filter(lambda r: r <= 0.5)\n",
    "    .count())\n",
    "\n",
    "tails = n - heads\n",
    "ratio = 1. * heads / n\n",
    "\n",
    "print('heads =', heads)\n",
    "print('tails =', tails)\n",
    "print('ratio =', ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd= sc.parallelize(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.88613435e-01, 4.70583534e-01, 9.40173231e-01, 6.06538206e-01,\n",
       "       5.82739900e-01, 3.10012393e-01, 5.73868817e-01, 6.89968386e-01,\n",
       "       8.05385543e-01, 8.31527851e-01, 3.73643538e-01, 6.83200513e-01,\n",
       "       9.87681135e-01, 5.97460448e-01, 2.06279091e-01, 1.16375958e-01,\n",
       "       6.22979381e-01, 3.88023201e-01, 4.06208955e-01, 4.02780758e-01,\n",
       "       1.26752278e-01, 4.71198052e-01, 1.38770302e-01, 9.55091253e-01,\n",
       "       1.46051682e-01, 3.66150949e-01, 6.30171247e-01, 3.38405921e-01,\n",
       "       7.36963580e-01, 6.49149873e-01, 6.22002556e-01, 6.83923157e-01,\n",
       "       5.88409289e-01, 3.41972056e-01, 1.73497383e-01, 2.16038453e-01,\n",
       "       9.26765960e-01, 6.04240841e-01, 1.58101826e-01, 6.70591572e-01,\n",
       "       6.25861799e-01, 6.76359928e-01, 4.68743415e-01, 9.07214895e-01,\n",
       "       7.03949369e-02, 5.67871888e-01, 1.51646617e-01, 9.89273783e-01,\n",
       "       4.70955478e-01, 8.33082300e-01, 6.06468677e-01, 2.73051158e-01,\n",
       "       8.41822357e-01, 2.91742545e-01, 5.17441674e-01, 2.61859311e-01,\n",
       "       9.84607488e-01, 5.65578251e-01, 4.75704360e-01, 4.82050409e-01,\n",
       "       2.28746520e-01, 8.96094290e-01, 4.60381808e-01, 6.66164369e-01,\n",
       "       7.50701818e-01, 6.71969180e-01, 4.44754014e-01, 6.12382995e-02,\n",
       "       6.30448470e-01, 4.31762481e-01, 8.74928748e-02, 4.61958077e-01,\n",
       "       8.14473629e-02, 1.27471594e-01, 1.01374105e-01, 9.79470968e-01,\n",
       "       8.01243971e-01, 6.00619437e-01, 3.59287370e-01, 5.88603691e-01,\n",
       "       6.01049341e-01, 9.89443626e-01, 4.90433252e-01, 4.95098968e-01,\n",
       "       9.81451822e-01, 1.60602555e-01, 6.12208593e-02, 5.69294790e-01,\n",
       "       1.57348476e-01, 2.57284133e-01, 2.04279537e-01, 3.51992427e-01,\n",
       "       8.17450862e-01, 5.73579863e-01, 4.84158613e-01, 6.33901810e-01,\n",
       "       9.52167627e-01, 9.80867876e-01, 3.53325901e-01, 6.83246986e-01,\n",
       "       2.97891024e-01, 7.13395036e-01, 5.50527984e-02, 4.25032233e-01,\n",
       "       7.78618378e-01, 9.18200042e-01, 9.99219952e-01, 7.78519522e-01,\n",
       "       2.88196389e-01, 1.60173135e-01, 9.20413111e-01, 5.71754476e-01,\n",
       "       9.89215529e-02, 2.67526058e-01, 2.74635875e-02, 8.14365374e-01,\n",
       "       5.02345896e-02, 1.35119525e-01, 3.52548697e-01, 5.90260282e-01,\n",
       "       8.14976831e-02, 4.65970143e-01, 5.31952943e-01, 2.58129131e-01,\n",
       "       6.33151300e-02, 5.08227039e-01, 7.80619087e-01, 8.22080711e-01,\n",
       "       5.23285249e-01, 8.74896889e-02, 6.93032218e-01, 9.89385351e-01,\n",
       "       9.95533829e-01, 3.95125234e-01, 8.25076028e-01, 8.44858027e-02,\n",
       "       6.75289477e-01, 3.32861287e-01, 4.05274440e-01, 4.00130384e-01,\n",
       "       6.14101186e-01, 5.71977650e-01, 1.51306509e-01, 1.48937528e-01,\n",
       "       5.32108484e-01, 8.57787592e-01, 4.06216592e-01, 6.69176001e-01,\n",
       "       6.87180109e-01, 1.08680928e-01, 1.64467696e-01, 6.55950077e-01,\n",
       "       4.57263318e-01, 7.94322160e-01, 6.50507008e-01, 7.04184812e-01,\n",
       "       6.87791734e-01, 3.05924062e-01, 6.82392631e-01, 6.00998266e-02,\n",
       "       6.20550269e-01, 7.07719247e-01, 9.39704706e-01, 5.85062233e-01,\n",
       "       4.13039223e-01, 6.19271267e-01, 4.40157277e-01, 9.04719594e-01,\n",
       "       8.06602698e-01, 3.52822942e-01, 2.33449651e-01, 5.48640722e-01,\n",
       "       6.86679429e-01, 5.90351343e-02, 4.60461834e-01, 6.88357434e-01,\n",
       "       7.52358498e-01, 7.00369426e-01, 5.18685623e-01, 9.30686886e-01,\n",
       "       7.33930449e-01, 7.45833914e-01, 6.35013854e-01, 2.11992796e-01,\n",
       "       4.68549870e-02, 8.46792266e-02, 3.00949460e-02, 5.33640582e-01,\n",
       "       3.18281293e-01, 2.38981901e-01, 1.73307719e-01, 6.16252421e-01,\n",
       "       3.51939313e-01, 2.33127037e-01, 4.92603506e-01, 2.83847371e-01,\n",
       "       3.80905322e-01, 3.08815141e-01, 4.53153061e-01, 1.95475441e-01,\n",
       "       9.99935216e-01, 3.87522385e-01, 6.42783438e-01, 8.05331680e-01,\n",
       "       8.89026114e-02, 9.06398885e-01, 1.51378569e-01, 6.53766504e-01,\n",
       "       6.74326317e-01, 4.58786099e-01, 1.92390331e-01, 6.46408892e-01,\n",
       "       1.64744356e-03, 9.94593178e-01, 4.28322534e-01, 4.84729013e-01,\n",
       "       6.18276092e-01, 1.85243089e-01, 1.76718993e-01, 3.95245897e-01,\n",
       "       2.07848400e-01, 6.60354578e-01, 2.70926308e-01, 2.40688801e-01,\n",
       "       3.83553270e-02, 1.70588949e-01, 1.06985784e-01, 2.28687938e-01,\n",
       "       4.10096800e-01, 9.50944591e-02, 3.45943843e-01, 7.35277558e-01,\n",
       "       5.86578781e-01, 6.27635926e-01, 6.84202648e-01, 6.53836995e-01,\n",
       "       8.19212251e-01, 9.13406348e-01, 6.24643228e-01, 4.46637601e-01,\n",
       "       9.45206955e-01, 4.03534670e-01, 8.51160061e-01, 2.80051165e-01,\n",
       "       2.93678047e-01, 9.80483281e-01, 9.80608701e-01, 9.94194538e-01,\n",
       "       6.91343366e-01, 3.07636522e-02, 2.41394558e-01, 5.24484772e-02,\n",
       "       4.45826646e-01, 1.40761207e-01, 6.27599008e-01, 4.34229663e-01,\n",
       "       6.38513205e-01, 6.90467957e-02, 5.07635394e-01, 9.20867770e-01,\n",
       "       5.08336854e-01, 2.72053550e-01, 9.22695801e-01, 2.15219510e-01,\n",
       "       9.38845854e-01, 5.45068733e-01, 5.60752678e-01, 1.52148694e-01,\n",
       "       1.58028920e-01, 8.31201262e-01, 6.61865328e-01, 1.77522941e-01,\n",
       "       4.72411789e-01, 6.23364558e-01, 7.38333423e-01, 7.02076644e-01,\n",
       "       8.80984013e-01, 7.90139331e-02, 9.72663119e-01, 4.87563200e-01,\n",
       "       1.18791440e-01, 9.85695787e-01, 7.31280606e-01, 9.63708397e-01,\n",
       "       8.67387887e-01, 2.59470580e-01, 3.85866634e-01, 2.09431998e-01,\n",
       "       8.50827605e-01, 9.47421913e-01, 3.60614453e-01, 9.26171034e-01,\n",
       "       4.38493356e-01, 1.87784711e-02, 3.30076010e-01, 1.26856289e-01,\n",
       "       7.83863883e-01, 3.12145775e-01, 3.78167304e-01, 8.07877584e-01,\n",
       "       3.65867604e-03, 7.24229256e-01, 5.20929821e-01, 5.31837103e-01,\n",
       "       4.12389907e-01, 9.00021137e-02, 8.15769893e-01, 9.25599266e-01,\n",
       "       4.46570525e-02, 1.11951843e-01, 5.97581674e-01, 3.36778154e-01,\n",
       "       7.61243974e-01, 4.74085092e-01, 6.46293729e-01, 8.36237901e-01,\n",
       "       7.09544296e-01, 1.80227144e-01, 1.86990100e-01, 2.81691636e-01,\n",
       "       6.39600020e-01, 6.31587406e-01, 8.76152880e-01, 3.85256995e-01,\n",
       "       4.64101601e-01, 2.34783685e-01, 2.12431218e-02, 5.73326628e-01,\n",
       "       2.62081369e-01, 5.20197196e-01, 6.10121663e-02, 3.59169436e-01,\n",
       "       5.82212113e-01, 2.52887777e-01, 3.95269506e-01, 1.91375462e-02,\n",
       "       3.88367016e-01, 9.45876889e-01, 5.39948434e-01, 7.46671587e-02,\n",
       "       1.05378946e-01, 8.77869861e-01, 7.35711304e-01, 1.37010227e-01,\n",
       "       4.85812526e-01, 3.31880622e-01, 3.21659413e-02, 9.83157006e-01,\n",
       "       9.31292975e-01, 5.31212064e-01, 6.35819875e-01, 8.64003425e-01,\n",
       "       2.35283947e-01, 8.81879751e-01, 6.41951657e-01, 2.55572567e-01,\n",
       "       1.92783407e-01, 3.27453244e-01, 3.27442881e-02, 4.55955845e-01,\n",
       "       1.91929850e-01, 2.94208554e-01, 6.30088433e-01, 7.06176144e-01,\n",
       "       1.97630890e-01, 8.31129278e-01, 3.15091474e-01, 1.65156396e-01,\n",
       "       3.94130962e-01, 5.30364695e-02, 5.38735171e-01, 5.64544624e-01,\n",
       "       8.41861484e-01, 3.92213055e-02, 9.91447288e-01, 9.32754189e-01,\n",
       "       7.83322405e-01, 5.55823572e-01, 9.42215854e-01, 2.29033594e-01,\n",
       "       4.31078311e-01, 3.09827471e-01, 6.19763310e-01, 1.19890575e-01,\n",
       "       7.83039809e-01, 6.30801519e-01, 6.82282278e-01, 7.35198156e-02,\n",
       "       5.68250029e-01, 6.52466931e-01, 4.88669169e-01, 5.64942956e-01,\n",
       "       4.25765298e-01, 3.91691363e-01, 4.32188168e-01, 2.29307681e-01,\n",
       "       6.16302646e-01, 2.88619311e-01, 1.34519842e-01, 7.26270337e-01,\n",
       "       2.62133423e-01, 4.25728452e-01, 3.60439541e-01, 5.48807430e-02,\n",
       "       8.80707599e-01, 4.53645135e-01, 3.59663737e-01, 1.23196240e-01,\n",
       "       7.16403573e-01, 1.45718848e-01, 2.65074232e-01, 3.58031142e-01,\n",
       "       7.80330522e-01, 4.62641166e-01, 1.05296011e-01, 1.67027956e-02,\n",
       "       3.83940814e-01, 8.82977871e-01, 6.03206636e-01, 4.52817819e-01,\n",
       "       9.98770125e-01, 8.12125935e-01, 5.75171097e-01, 9.39941766e-01,\n",
       "       2.49435134e-01, 3.49836121e-01, 6.20360191e-01, 1.31890179e-01,\n",
       "       3.32036796e-01, 8.67672230e-01, 6.70628124e-01, 4.40311932e-01,\n",
       "       8.39831208e-01, 7.34787289e-01, 7.61461990e-01, 9.26683329e-01,\n",
       "       7.55783126e-01, 4.48758734e-01, 8.75602492e-01, 2.17854391e-01,\n",
       "       9.09014214e-01, 5.89508807e-01, 8.13471554e-02, 5.51129299e-01,\n",
       "       4.42386931e-01, 1.94099147e-01, 2.24844280e-01, 2.45586925e-01,\n",
       "       4.52037645e-01, 8.73122655e-01, 1.04736138e-02, 8.93859580e-01,\n",
       "       9.79724073e-01, 8.29109225e-01, 4.62538928e-01, 6.41854647e-01,\n",
       "       5.28913327e-01, 8.47055898e-01, 7.49058881e-01, 4.73842517e-01,\n",
       "       9.57076475e-01, 9.21765964e-01, 9.19109373e-01, 8.04110330e-01,\n",
       "       8.64050326e-01, 1.48512610e-01, 7.19595750e-01, 1.15882981e-01,\n",
       "       8.29391041e-01, 9.27165360e-01, 7.59439634e-01, 7.63472584e-01,\n",
       "       9.63968715e-01, 1.47349453e-01, 6.38370510e-01, 8.49522199e-01,\n",
       "       2.51410165e-01, 3.67648311e-01, 8.39718029e-02, 8.88701978e-01,\n",
       "       6.34676635e-01, 5.89177803e-01, 6.47318134e-01, 7.91479148e-01,\n",
       "       9.05595844e-01, 2.95253267e-01, 3.86624863e-02, 8.79801606e-01,\n",
       "       2.98056804e-02, 5.72336303e-01, 5.68050571e-01, 8.30636143e-01,\n",
       "       8.32775051e-01, 2.18442861e-01, 4.49226386e-01, 8.29017676e-01,\n",
       "       1.42270453e-01, 2.95558731e-01, 5.86511664e-01, 3.52378417e-01,\n",
       "       8.28625760e-01, 5.60660642e-02, 3.71214953e-01, 6.71492759e-01,\n",
       "       6.86768470e-01, 5.46439303e-01, 2.24192431e-01, 7.44927515e-01,\n",
       "       4.05244937e-01, 1.14752713e-01, 7.87948000e-01, 2.33660308e-01,\n",
       "       7.85551244e-01, 9.22876405e-01, 9.56823160e-01, 1.97473620e-01,\n",
       "       5.62589867e-01, 3.08720809e-01, 6.64391393e-01, 3.57668510e-01,\n",
       "       9.12650426e-01, 1.21574650e-01, 7.30376968e-01, 7.67883952e-01,\n",
       "       8.56805476e-01, 3.73455639e-01, 4.73554745e-01, 1.85926599e-01,\n",
       "       4.80640139e-01, 5.21770920e-02, 6.22359267e-01, 1.85823283e-01,\n",
       "       8.03162939e-01, 3.00705157e-01, 6.39302957e-01, 2.38558515e-01,\n",
       "       5.03618796e-01, 9.21318509e-01, 4.86226752e-01, 9.55355225e-01,\n",
       "       5.97798650e-02, 2.07481055e-01, 1.45296805e-01, 5.58625983e-01,\n",
       "       9.47828791e-01, 3.52803108e-02, 7.43108304e-01, 3.32198616e-01,\n",
       "       8.28564682e-01, 1.40720897e-01, 3.28340632e-01, 6.50556860e-02,\n",
       "       4.85692674e-01, 4.35475822e-02, 6.74264855e-01, 1.22890799e-01,\n",
       "       7.89404506e-01, 3.19481448e-01, 4.43948362e-01, 2.45294040e-01,\n",
       "       2.17810395e-01, 4.24774737e-02, 1.15583882e-01, 7.50984029e-01,\n",
       "       8.07151768e-01, 7.56736655e-01, 8.35461541e-01, 4.54989925e-01,\n",
       "       7.62577575e-01, 4.83651016e-01, 4.14355087e-01, 7.52057168e-01,\n",
       "       4.62161820e-01, 7.70237973e-01, 6.25076598e-01, 2.90368335e-01,\n",
       "       6.41032462e-01, 7.87138856e-01, 6.54097999e-01, 4.96780833e-01,\n",
       "       4.52974920e-01, 2.97211397e-01, 9.93424325e-01, 2.93520969e-01,\n",
       "       6.13743574e-01, 5.31808776e-01, 1.60249717e-01, 8.83958470e-01,\n",
       "       3.46746650e-01, 2.27333892e-02, 6.64932215e-02, 4.73075117e-01,\n",
       "       2.87473589e-01, 9.61368030e-01, 7.68572615e-01, 9.61333434e-01,\n",
       "       1.08655336e-01, 8.78261977e-01, 4.69925880e-02, 8.27712959e-01,\n",
       "       7.51284972e-01, 6.13727338e-01, 1.93654868e-01, 2.11503194e-01,\n",
       "       2.60847170e-01, 4.78309606e-01, 4.18727707e-01, 8.71309048e-01,\n",
       "       1.27313611e-01, 8.64854391e-01, 7.78048161e-01, 1.96603632e-01,\n",
       "       2.59420997e-01, 3.83537865e-01, 2.43980375e-01, 4.24162978e-01,\n",
       "       2.11515125e-02, 1.29604652e-01, 2.48543622e-01, 7.28579993e-01,\n",
       "       8.17950275e-01, 3.73480287e-01, 4.04348903e-01, 7.43305289e-01,\n",
       "       2.30364502e-01, 2.44187892e-01, 8.37121334e-01, 8.35309410e-01,\n",
       "       1.11724404e-01, 3.58624200e-01, 6.47606323e-01, 7.33777546e-01,\n",
       "       1.63412126e-01, 5.32618819e-01, 9.99825015e-01, 3.56109553e-01,\n",
       "       6.21378278e-01, 7.89412301e-01, 8.03685656e-01, 1.36972210e-02,\n",
       "       1.40190610e-01, 3.30189456e-02, 3.93516743e-01, 8.13078375e-02,\n",
       "       1.56399221e-01, 8.75850771e-01, 9.64363874e-01, 1.22353441e-01,\n",
       "       2.83745079e-01, 9.98647834e-01, 5.71460851e-01, 5.88201892e-01,\n",
       "       3.38994810e-01, 3.56375053e-01, 7.00592854e-01, 7.44168790e-01,\n",
       "       7.87567202e-01, 6.76920127e-01, 1.37627912e-01, 5.16616960e-01,\n",
       "       8.04168322e-01, 2.79973022e-01, 6.60640818e-01, 1.82024170e-01,\n",
       "       4.54527662e-01, 5.78542273e-01, 9.49933434e-03, 6.05960656e-01,\n",
       "       6.21114324e-02, 5.29327353e-01, 5.91055818e-02, 3.27605089e-01,\n",
       "       3.73113821e-01, 2.16290474e-01, 6.48971732e-01, 1.47400765e-01,\n",
       "       1.94208122e-01, 4.72875628e-01, 8.60804820e-01, 3.49570895e-01,\n",
       "       8.75819957e-01, 2.34078118e-01, 7.20663414e-01, 9.69144814e-02,\n",
       "       6.56205180e-01, 7.48853059e-01, 4.94603749e-01, 3.20645614e-01,\n",
       "       9.60419047e-01, 1.06624907e-01, 8.20623567e-01, 1.29507950e-01,\n",
       "       5.45890615e-01, 2.11490673e-02, 1.95906977e-01, 1.57674368e-01,\n",
       "       9.87808199e-01, 6.20875304e-01, 8.11540556e-01, 8.60342032e-02,\n",
       "       9.57846370e-01, 4.65385824e-01, 6.31921826e-01, 8.52302268e-01,\n",
       "       2.78235880e-01, 3.16958367e-01, 8.49975922e-01, 4.67632934e-01,\n",
       "       9.37588838e-01, 8.13728767e-01, 2.19704167e-01, 6.51757143e-01,\n",
       "       8.24732022e-02, 5.17845130e-01, 9.74116806e-02, 4.08071215e-01,\n",
       "       4.11779947e-01, 4.92713797e-01, 9.67330491e-01, 9.92221272e-04,\n",
       "       6.31125041e-01, 6.74974936e-01, 6.17656876e-02, 8.31487132e-01,\n",
       "       6.08133575e-01, 9.12013857e-01, 3.31543217e-01, 8.34012869e-01,\n",
       "       1.40356135e-01, 7.74589560e-01, 9.86545642e-01, 6.19532233e-01,\n",
       "       9.53168200e-01, 9.84153006e-01, 3.35397248e-01, 8.68339969e-01,\n",
       "       8.53374182e-01, 7.94390462e-02, 5.95135915e-01, 7.44639565e-01,\n",
       "       8.04521555e-01, 5.39828727e-01, 2.86335027e-01, 4.13033322e-01,\n",
       "       4.18903592e-01, 9.49740745e-01, 2.49955746e-01, 1.95940188e-01,\n",
       "       1.40205521e-01, 8.75995549e-01, 8.98074431e-02, 4.93848989e-01,\n",
       "       4.80721776e-01, 1.13235683e-01, 8.95651216e-02, 7.44791905e-01,\n",
       "       7.26189395e-01, 5.22481683e-01, 7.89516190e-01, 2.67107159e-01,\n",
       "       7.89728010e-01, 5.43808608e-01, 3.17291792e-01, 5.58807346e-01,\n",
       "       5.90648542e-01, 7.36475047e-01, 5.92046435e-01, 8.87950222e-01,\n",
       "       3.94306864e-01, 9.40618946e-01, 6.74714759e-02, 5.78158427e-01,\n",
       "       8.91087915e-01, 1.59979030e-01, 4.52233493e-01, 5.80008742e-01,\n",
       "       8.70184803e-01, 8.23292898e-01, 6.39609974e-01, 2.83081334e-01,\n",
       "       2.75865250e-01, 6.46553632e-01, 1.79943673e-01, 3.41610471e-01,\n",
       "       2.99340684e-01, 1.86147380e-01, 2.52457447e-01, 8.29202920e-02,\n",
       "       9.87098299e-01, 8.29557381e-01, 9.51245902e-01, 6.50830700e-01,\n",
       "       6.42470589e-01, 6.19615913e-01, 3.60251271e-01, 1.79124516e-01,\n",
       "       7.95588745e-01, 8.31036569e-01, 5.74132069e-01, 6.82147382e-01,\n",
       "       7.70736265e-02, 2.89813829e-01, 2.48544638e-01, 4.78347701e-02,\n",
       "       5.92944301e-01, 4.00966757e-01, 4.38014963e-01, 5.19954869e-01,\n",
       "       7.33223759e-01, 9.34358419e-02, 4.92145137e-01, 8.05018798e-01,\n",
       "       4.23360614e-01, 4.93698528e-01, 9.94252260e-01, 5.34334420e-01,\n",
       "       4.63516528e-01, 1.00775752e-01, 9.75058295e-01, 9.26976441e-01,\n",
       "       6.54888740e-01, 7.29408750e-02, 4.66427814e-01, 9.82121225e-01,\n",
       "       1.75722440e-01, 1.43167570e-01, 2.69808751e-01, 6.09705990e-01,\n",
       "       9.33181115e-01, 2.22929377e-01, 5.32665736e-01, 9.80919289e-01,\n",
       "       3.66618259e-01, 2.43330314e-01, 9.82559220e-01, 3.75406548e-01,\n",
       "       9.72865177e-02, 8.55511253e-01, 7.34808000e-01, 1.94456702e-02,\n",
       "       6.68633715e-01, 5.28457632e-01, 7.23727883e-01, 2.42101099e-01,\n",
       "       9.15844992e-01, 3.31609725e-01, 2.80097700e-01, 8.81523625e-01,\n",
       "       7.45002455e-02, 6.58483398e-01, 4.11813849e-01, 3.45622973e-01,\n",
       "       6.32291630e-01, 7.84110868e-01, 4.50091682e-01, 8.40205437e-01,\n",
       "       2.12580308e-01, 4.22445414e-01, 3.55848080e-02, 4.22558597e-01,\n",
       "       8.37500305e-01, 1.29596340e-01, 3.26961250e-02, 4.53559505e-01,\n",
       "       3.17967706e-01, 7.19547965e-01, 3.05355915e-01, 6.59205722e-01,\n",
       "       7.33600824e-01, 5.47954966e-03, 3.89711058e-01, 5.08694346e-01,\n",
       "       6.79452757e-01, 6.72931939e-01, 7.36099867e-01, 1.79162036e-01,\n",
       "       4.23898765e-01, 7.11148507e-01, 7.76749172e-01, 6.94855089e-01,\n",
       "       8.10969883e-01, 7.33437321e-01, 2.69999906e-01, 9.32606695e-01,\n",
       "       8.90633755e-01, 1.41409132e-01, 6.25901927e-02, 4.92990387e-01,\n",
       "       7.70756605e-01, 6.13438652e-02, 8.33027084e-01, 5.55055350e-01,\n",
       "       6.55529598e-01, 5.41689962e-01, 1.83254852e-01, 1.62149177e-01,\n",
       "       9.73157568e-02, 5.46190888e-01, 2.20327318e-01, 8.55764163e-01,\n",
       "       6.67563925e-02, 4.21105222e-01, 2.00780612e-01, 1.36240918e-01,\n",
       "       6.06743815e-01, 2.78489612e-01, 7.53565561e-01, 6.55319513e-02,\n",
       "       7.12626475e-01, 7.31810773e-01, 3.07283408e-01, 4.72591064e-01,\n",
       "       4.55720939e-01, 4.74565986e-01, 1.67744982e-01, 7.24763108e-01,\n",
       "       9.76760452e-01, 4.66646241e-01, 7.17123741e-01, 6.60332718e-01,\n",
       "       5.63210615e-01, 8.43683929e-01, 4.86221810e-01, 4.98406791e-01,\n",
       "       3.28200347e-01, 7.22272474e-01, 7.64016021e-01, 9.57852182e-01,\n",
       "       3.74763399e-01, 8.12014961e-01, 2.33458533e-01, 8.04874857e-01,\n",
       "       5.16928347e-01, 7.59674304e-01, 4.36813674e-01, 6.67307592e-01,\n",
       "       6.31365659e-01, 5.81241158e-01, 6.50051686e-01, 4.29909150e-01,\n",
       "       2.96813354e-01, 8.77803976e-01, 6.05014416e-02, 9.89542294e-01,\n",
       "       3.49578242e-01, 1.81317321e-01, 5.34656380e-01, 7.50189233e-01,\n",
       "       7.45047875e-01, 8.92879676e-01, 6.68069305e-01, 7.75304324e-01,\n",
       "       4.53290761e-01, 6.82424317e-01, 4.51639502e-01, 6.81431947e-01,\n",
       "       5.96872994e-01, 6.24746506e-01, 7.84730839e-01, 8.78328133e-01,\n",
       "       4.98857287e-01, 3.27140689e-02, 5.45000473e-01, 8.30974257e-01,\n",
       "       5.94806647e-01, 1.97030847e-01, 2.49852170e-01, 9.39463914e-01,\n",
       "       1.58849226e-01, 4.20545841e-01, 8.24429529e-01, 9.31105666e-01,\n",
       "       9.99228805e-01, 6.30466021e-01, 2.03133080e-01, 2.35626992e-01,\n",
       "       9.02504476e-01, 9.56574816e-01, 3.40884330e-01, 2.33050962e-01,\n",
       "       6.01684063e-01, 1.19599788e-01, 6.81824123e-01, 5.96204967e-02,\n",
       "       5.79618430e-01, 8.31502590e-01, 2.28705689e-01, 8.46556959e-01,\n",
       "       6.39576820e-01, 6.59800176e-02, 9.10990271e-01, 9.84634755e-01,\n",
       "       5.27239506e-01, 1.78720282e-01, 5.29235168e-01, 1.82860213e-01,\n",
       "       5.31626668e-01, 5.16232730e-01, 4.72380059e-01, 5.74946561e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.filter(lambda r: r <= 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count() #action : collect, count, mean #tranformation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- `sc.parallelize` creates an RDD.\n",
    "- `map` and `filter` are *transformations*.\n",
    "  - They create new RDDs from existing RDDs.\n",
    "- `count` is an *action* and brings the data from the RDDs back to the driver.\n",
    "\n",
    "## Spark Terminology\n",
    "\n",
    "Term | Meaning\n",
    "--- | ---\n",
    "RDD | *Resilient Distributed Dataset* or a distributed sequence of records\n",
    "Spark Job | Sequence of transformations on data with a final action\n",
    "Transformation | Spark operation that produces an RDD\n",
    "Action | Spark operation that produces a local object\n",
    "Spark Application | Sequence of Spark jobs and other code\n",
    "\n",
    "- A Spark job pushes the data to the cluster, all computation happens on the *executors*, then the result is sent back to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "In this Spark job, what is the transformation and what is the action?\n",
    "<br>\n",
    "`sc.parallelize(xrange(10)).filter(lambda x: x % 2 == 0).collect()`\n",
    "</summary>\n",
    "1. `filter` is the transformation.\n",
    "<br>\n",
    "2. `collect` is the action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda vs. Functions\n",
    "\n",
    "- Instead of `lambda` you can pass in fully defined functions into `map`, `filter`, and other RDD transformations.\n",
    "- Use `lambda` for short functions.\n",
    "- Use `def` for more substantial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Primes\n",
    "\n",
    "Q: Find all the primes less than 100.\n",
    "\n",
    "- Define function to determine if a number is prime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number ** 0.5) + 1\n",
    "    for factor in range(factor_min, factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use this to filter out non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "numbers = range(2, 100)\n",
    "\n",
    "primes = (sc.parallelize(numbers)\n",
    "    .filter(is_prime)\n",
    "    .collect())\n",
    "\n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers=range(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd.filter(is_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.filter(lambda x: x>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "<details>\n",
    "<summary>Q: Where does `is_prime` execute?</summary>\n",
    "A: On the executors.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: Where does is the RDD object collected?</summary>\n",
    "A: On the driver.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "- Common RDD Constructors\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`sc.parallelize(list)` | Create RDD of elements of list\n",
    "`sc.textFile(path)` | Create RDD of lines from file\n",
    "\n",
    "- Common Transformations\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`filter(lambda x: x % 2 == 0)` | Discard non-even elements\n",
    "`map(lambda x: x * 2)` | Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())` | Split each string into words\n",
    "`flatMap(lambda x: x.split())` | Split each string into words and flatten sequence\n",
    "`sample(withReplacement = True, 0.25)` | Create sample of 25% of elements with replacement\n",
    "`union(rdd)` | Append `rdd` to existing RDD\n",
    "`distinct()` | Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending = False)` | Sort elements in descending order\n",
    "\n",
    "- Common Actions\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`collect()` | Convert RDD to in-memory list \n",
    "`take(3)` | First 3 elements of RDD \n",
    "`top(3)` | Top 3 elements of RDD\n",
    "`takeSample(withReplacement = True, 3)` | Create sample of 3 elements with replacement\n",
    "`sum()` | Find element sum (assumes numeric elements)\n",
    "`mean()` | Find element mean (assumes numeric elements)\n",
    "`stdev()` | Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 5, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 3, 2, 2, 1, 4, 5]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6, 5, 4, 3, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(2, 10)).sortBy(lambda x: x, ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt').map(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt').flatMap(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map vs. FlatMap\n",
    "\n",
    "Here's the difference between `map` and `flatMap`:\n",
    "\n",
    "- Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'],\n",
       " ['another', 'line'],\n",
       " ['yet', 'another', 'line'],\n",
       " ['yet', 'another', 'another', 'line']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd= sc.textFile('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'],\n",
       " ['another', 'line'],\n",
       " ['yet', 'another', 'line'],\n",
       " ['yet', 'another', 'another', 'line']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FlatMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " 'another',\n",
       " 'line',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'line',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'another',\n",
       " 'line']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we have an RDD containing sales transactions we can find the total revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['106    11/19/2014     202     CA     331        330.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt').top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300.00', '450.00', '200.00']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300.00', '450.00', '200.00', '330.00', '750.00', '200.00']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create tuples of states and revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 300.0),\n",
       " ('OR', 450.0),\n",
       " ('CA', 200.0),\n",
       " ('CA', 330.0),\n",
       " ('WA', 750.0),\n",
       " ('CA', 200.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 730.0), ('WA', 1050.0), ('OR', 450.0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 1050.0), ('CA', 730.0), ('OR', 450.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the states by Amount in descending order\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1], ascending=False)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1])\\\n",
    "    .map(lambda x: x[1])\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What does `reduceByKey` do?</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values across all tuples with the same key by using the function we pass to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "## Notes\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "- It requires that the operation is associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "Q: Implement word count in Spark.\n",
    "\n",
    "- Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('line', 3), ('yet', 2), ('hello', 1), ('another', 4)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2= rdd.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1),\n",
       " ('world', 1),\n",
       " ('another', 1),\n",
       " ('line', 1),\n",
       " ('yet', 1),\n",
       " ('another', 1),\n",
       " ('line', 1),\n",
       " ('yet', 1),\n",
       " ('another', 1),\n",
       " ('another', 1),\n",
       " ('line', 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('line', 3), ('yet', 2), ('hello', 1), ('another', 4)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.reduceByKey(lambda a, b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making List Indexing Readable\n",
    "\n",
    "- While this code looks reasonable, the list indexes are cryptic and hard to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1], ascending = False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make this more readable using Python's argument unpacking feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Unpacking\n",
    "\n",
    "Q: Which version of `getCity` is more readable and why?\n",
    "\n",
    "- Consider this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getCity1(client) = SF\n",
      "getCity1(client) = SF\n"
     ]
    }
   ],
   "source": [
    "client = ('Dmitri', 'Smith', 'SF')\n",
    "\n",
    "def getCity1(client):\n",
    "    return client[2]\n",
    "\n",
    "def getCity2((first, last, city)):\n",
    "    return city\n",
    "\n",
    "print 'getCity1(client) =', getCity1(client)\n",
    "print 'getCity1(client) =', getCity2(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between `getCity1` and `getCity2`?\n",
    "- Which is more readable?\n",
    "- What is the essence of argument unpacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Can argument unpacking work for deeper nested structures?</summary>\n",
    "A: Yes. It can work for arbitrarily nested tuples and lists.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How would you write `getCity` given\n",
    "<br>\n",
    "`client = ('Dmitri','Smith', ('123 Eddy','SF','CA'))`\n",
    "</summary>\n",
    "`def getCity((first, last, (street, city, state))): return city`\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = ('Dmitri', 'Smith',('123 Eddy', 'SF', 'CA'))\n",
    "\n",
    "def getCity((first, last, (street, city, state))):\n",
    "    return city\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you find yourself indexing into a tuple consider usingargument unpacking to make it more readable.\n",
    "- Here is what `getCity` looks like without tuple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def badGetCity(client):\n",
    "    return client[2][1]\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Unpacking In Spark\n",
    "\n",
    "Q: Rewrite the last Spark job using argument unpacking.\n",
    "\n",
    "- Here is the original version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code with argument unpacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda (id, date, store, state, product, amount): (state, float(amount)))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda (state, amount): amount, ascending = False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case because we have a long list or tuple argument unpacking is a judgement call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey\n",
    "\n",
    "`reduceByKey` lets us aggregate values using sum, max, min, and other associative operations. But what about non-associative operations like average? How can we calculate them?\n",
    "\n",
    "There are several ways to do this.\n",
    "- The first approach is to change the RDD tuples so that the operation becomes associative. \n",
    "  - Instead of `(state, amount)` use `(state, (amount, count))`.\n",
    "- The second approach is to use `groupByKey`, which is like `reduceByKey` except it gathers together all the values in an iterator. \n",
    "  - The iterator can then be reduced in a `map` step immediately after the `groupByKey`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Calculate the average sales per state.\n",
    "\n",
    "- Approach 1: Restructure the tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 243.33333333333334), (u'WA', 525.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], (float(x[-1]), 1)))\\\n",
    "    .reduceByKey(lambda (amount1, count1), (amount2, count2):\\\n",
    "        (amount1 + amount2, count1 + count2))\\\n",
    "    .map(lambda (state, (amount, count)): (state, amount / count))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the argument unpacking we are doing in `reduceByKey` to name the elements of the tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approach 2: Use `groupByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 243.33333333333334), (u'WA', 525.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(iterator):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return total / count\n",
    "\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda (state, iterator): (state, mean(iterator)))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are using unpacking again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What would be the disadvantage of not using unpacking?</summary>\n",
    "1. We will need to drill down into the elements.\n",
    "<br>\n",
    "2. The code will be harder to read.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What are the pros and cons of `reduceByKey` vs. `groupByKey`?</summary>\n",
    "1. `groupByKey` stores the values for particular key as an iterable.\n",
    "<br>\n",
    "2. This will take up space in memory or on disk.\n",
    "<br>\n",
    "3. `reduceByKey` therefore is more scalable.\n",
    "<br>\n",
    "4. However, `groupByKey` does not require associative reducer operation.\n",
    "<br>\n",
    "5. For this reason `groupByKey` can be easier to program with.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Q: Given a table of employees and locations find the cities that the employees live in.\n",
    "\n",
    "- The easiest way to do this is with a `join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, ('Dee', None)),\n",
       " (14, ('Alice', 'SF')),\n",
       " (14, ('Chad', 'SF')),\n",
       " (15, ('Bob', 'Seattle')),\n",
       " (15, ('Jen', 'Seattle'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Employees: emp_id, loc_id, name\n",
    "employee_data = [\n",
    "    (101, 14, 'Alice'),\n",
    "    (102, 15, 'Bob'),\n",
    "    (103, 14, 'Chad'),\n",
    "    (104, 15, 'Jen'),\n",
    "    (105, 13, 'Dee') ]\n",
    "\n",
    "# Locations: loc_id, location\n",
    "location_data = [\n",
    "    (14, 'SF'),\n",
    "    (15, 'Seattle'),\n",
    "    (16, 'Portland')]\n",
    "\n",
    "employees = sc.parallelize(employee_data)\n",
    "locations = sc.parallelize(location_data)\n",
    "\n",
    "# Re-key employee records with loc_id\n",
    "employees2 = employees.map(lambda (emp_id, loc_id, name): (loc_id, name))\n",
    "\n",
    "# Now join.\n",
    "employees2.leftOuterJoin(locations).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: How can we keep employees that don't have a valid location ID in the final result?</summary>\n",
    "1. Use `leftOuterJoin` to keep employees without location IDs.\n",
    "<br>\n",
    "2. Use `rightOuterJoin` to keep locations without employees. \n",
    "<br>\n",
    "3. Use `fullOuterJoin` to keep both.\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Statistics\n",
    "\n",
    "Q: How would you calculate the mean, variance, and standard deviation of a sample produced by Python's `random` function?\n",
    "\n",
    "- Create an RDD and apply the statistical actions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean     = 0.506333143283\n",
      "variance = 0.0813565893047\n",
      "stdev    = 0.285230765004\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "list = [random.random() for _ in xrange(n)]\n",
    "\n",
    "rdd = sc.parallelize(list)\n",
    "\n",
    "print 'mean     =', rdd.mean()\n",
    "print 'variance =', rdd.variance()\n",
    "print 'stdev    =', rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What requirement does an RDD have to satisfy before you can apply these statistical actions to it? \n",
    "</summary>\n",
    "The RDD must consist of numeric elements.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is the advantage of using Spark vs Numpy to calculate mean or standard deviation?</summary>\n",
    "The calculation is distributed across different machines and will be more scalable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Laziness\n",
    "\n",
    "- Q: What is this Spark job doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 5.38 ms, total: 18.1 ms\n",
      "Wall time: 1.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000000\n",
    "\n",
    "%time sc.parallelize(xrange(n)).map(lambda x: x + 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: How is the following job different from the previous one? How\n",
    "  long do you expect it to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 ms, sys: 1.84 ms, total: 4.36 ms\n",
      "Wall time: 7.46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[210] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sc.parallelize(xrange(n)).map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Why did the second job complete so much faster?</summary>\n",
    "1. Because Spark is lazy. \n",
    "<br>\n",
    "2. Transformations produce new RDDs and do no operations on the data.\n",
    "<br>\n",
    "3. Nothing happens until an action is applied to an RDD.\n",
    "<br>\n",
    "4. An RDD is the *recipe* for a transformation, rather than the *result* of the transformation.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is the benefit of keeping the recipe instead of the result of the action?</summary>\n",
    "1. It saves memory.\n",
    "<br>\n",
    "2. It produces *resilience*. \n",
    "<br>\n",
    "3. If an RDD loses data on a machine, it always knows how to recompute it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "Besides reading data Spark and also write data out to a file system.\n",
    "\n",
    "Q: Calculate the squares of integers from 1 to 10 and write them out to `squares.txt`.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(xrange(10))\n",
    "rdd2 = rdd1.map(lambda x: x ** 2)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: squares.txt: Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the output is a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  0 Jun  8 22:13 _SUCCESS\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00000\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  7 Jun  8 22:13 part-00001\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00002\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  9 Jun  8 22:13 part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squares.txt/part-00000\r\n",
      "0\r\n",
      "1\r\n",
      "squares.txt/part-00001\r\n",
      "4\r\n",
      "9\r\n",
      "16\r\n",
      "squares.txt/part-00002\r\n",
      "25\r\n",
      "36\r\n",
      "squares.txt/part-00003\r\n",
      "49\r\n",
      "64\r\n",
      "81\r\n"
     ]
    }
   ],
   "source": [
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What's going on? Why are there four files (excluding `_SUCCESS`) in the output directory?</summary>\n",
    "1. There were four threads that were processing the RDD.\n",
    "<br>\n",
    "2. The RDD was split up in four partitions (default with local mode: number of cores on the local machine).\n",
    "<br>\n",
    "3. Each partition was processed in a different task.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Q: Can we control the number of partitions/tasks that Spark uses for processing data? Solve the same problem as above but this time with 5 tasks.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 5\n",
    "rdd1 = sc.parallelize(xrange(10), partitions)\n",
    "rdd2 = rdd1.map(lambda x: x ** 2)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  0 Jun  8 22:13 _SUCCESS\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00000\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00001\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00002\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00003\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00004\n",
      "squares.txt/part-00000\n",
      "0\n",
      "1\n",
      "squares.txt/part-00001\n",
      "4\n",
      "9\n",
      "squares.txt/part-00002\n",
      "16\n",
      "25\n",
      "squares.txt/part-00003\n",
      "36\n",
      "49\n",
      "squares.txt/part-00004\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "!ls -l squares.txt\n",
    "\n",
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many partitions does Spark use by default?</summary>\n",
    "1. For operations like parallelize, it depends on the cluster manager:\n",
    "<br>\n",
    "  - Local mode: number of cores on the local machine\n",
    "<br>\n",
    "  - Others: total number of cores on all executor nodes or 2, whichever is larger\n",
    "<br>\n",
    "2. If you read an HDFS file into an RDD, Spark uses one partition per block.\n",
    "<br>\n",
    "3. If you read a file into an RDD from S3 or some other source, Spark uses 1 partition per 32 MB of data.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: If I read a file that is 200 MB into an RDD, how many partitions will that have?</summary>\n",
    "1. If the file is on HDFS that will produce 2 partitions (each is 128 MB).\n",
    "<br>\n",
    "2. If the file is on S3 or some other file system it will produce 7 partitions.\n",
    "<br>\n",
    "3. You can also control the number of partitions by passing in an additional argument into `textFile`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Terminology\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "Term | Meaning\n",
    "--- |---\n",
    "Task | Single thread in an executor\n",
    "Partition | Data processed by a single task\n",
    "Record | Records make up a partition that is processed by a single task\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Every Spark application gets executors when you create a new `SparkContext`.\n",
    "- You can specify how many cores to assign to each executor.\n",
    "- A core is equivalent to a thread.\n",
    "- The number of cores determine how many tasks can run concurrently on an executor.\n",
    "- Each task corresponds to one partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Q: Suppose you have 2 executors, each with 2 cores--so a total of 4\n",
    "cores. And you start a Spark job with 8 partitions. How many tasks\n",
    "will run concurrently?\n",
    "</summary>\n",
    "4 tasks will execute concurrently.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What happens to the other partitions?</summary>\n",
    "1. The other partitions wait in queue until a task thread becomes available.\n",
    "<br>\n",
    "2. Think of cores as turnstile gates at a train station, and partitions as people.\n",
    "<br>\n",
    "3. The number of turnstiles determine how many people can get through at once.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many Spark jobs can you have in a Spark application?</summary>\n",
    "As many as you want.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many Spark applications and Spark jobs are in this IPython Notebook?</summary>\n",
    "1. There is one Spark application because there is one `SparkContext`.\n",
    "<br>\n",
    "2. There are as many Spark jobs as we have invoked actions on RDDs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Quotes\n",
    "\n",
    "Q: Find the date on which AAPL's stock price was the highest.\n",
    "\n",
    "Suppose you have stock market data from Yahoo! for AAPL from <http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices>. The data is in CSV format and has these values.\n",
    "\n",
    "Date | Open | High | Low | Close | Volume | Adj Close\n",
    "---- | --- | --- | --- | -----   |------ | ---\n",
    "11-18-2014 | 113.94 | 115.69 | 113.89 | 115.47 | 44,200,30 | 115.47\n",
    "11-17-2014 | 114.27 | 117.28 | 113.30 | 113.99 | 46,746,700 | 113.99\n",
    "\n",
    "Here is what the CSV looks like:\n",
    "    \n",
    "    csv = [\n",
    "      \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "      \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "      \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "    ]\n",
    "\n",
    "Lets find the date on which the price was the highest. \n",
    "\n",
    "<details>\n",
    "<summary>Q: What two fields do we need to extract?</summary>\n",
    "1. *Date* and *Adj Close*.\n",
    "<br>\n",
    "2. We want to use *Adj Close* instead of *High* so our calculation is not affected by stock splits.\n",
    "</details>\n",
    "\n",
    "<details><summary>Q: What field should we sort on?</summary>\n",
    "*Adj Close*\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What sequence of operations would we need to perform?</summary>\n",
    "1. Use `filter` to remove the header line.\n",
    "<br>\n",
    "2. Use `map` to split each row into fields.\n",
    "<br>\n",
    "3. Use `map` to extract *Adj Close* and *Date*.\n",
    "<br>\n",
    "4. Use `sortBy` to sort descending on *Adj Close*.\n",
    "<br>\n",
    "5. Use `take(1)` to get the highest value.\n",
    "</details>\n",
    "\n",
    "- Here is full source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(115.47, '2014-11-18')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = [\n",
    "  \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "  \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "  \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "]\n",
    "\n",
    "sc.parallelize(csv) \\\n",
    "  .filter(lambda line: not line.startswith(\"#\")) \\\n",
    "  .map(lambda line: line.split(\",\")) \\\n",
    "  .map(lambda fields: (float(fields[-1]), fields[0])) \\\n",
    "  .sortBy(lambda (close, date): close, ascending = False) \\\n",
    "  .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the program for finding the high of any stock that stores\n",
    "  the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "# import re\n",
    "\n",
    "# def get_stock_high(symbol):\n",
    "    \n",
    "#     url = 'http://real-chart.finance.yahoo.com/table.csv?s=' + symbol + '&g=d&ignore=.csv'\n",
    "#     csv = urllib2.urlopen(url).read()\n",
    "#     csv_lines = csv.split('\\n')\n",
    "\n",
    "#     #print csv_lines\n",
    "#     stock_rdd = sc.parallelize(csv_lines)\\\n",
    "#         .filter(lambda line: re.match(r'\\d', line))\\\n",
    "#         .map(lambda line: line.split(\",\"))\\\n",
    "#         .map(lambda fields: (float(fields[-1]), fields[0]))\\\n",
    "#         .sortBy(lambda (close, date): close, ascending = False)\n",
    "\n",
    "#     return stock_rdd.take(1)\n",
    "\n",
    "# get_stock_high('AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Spark is high-level like Hive and Pig.\n",
    "- At the same time it does not invent a new language.\n",
    "- This allows it to leverage the ecosystem of tools that Python, Scala, and Java provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Caching\n",
    "\n",
    "- Consider this Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 500000\n",
    "numbers = [random.random() for _ in xrange(n)]\n",
    "rdd1 = sc.parallelize(numbers)\n",
    "rdd2 = rdd1.sortBy(lambda number: number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets time running `count()` on `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use `cache()` to make Spark cache the RDD.\n",
    "\n",
    "- Let's cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 ms, sys: 2.38 ms, total: 8.58 ms\n",
      "Wall time: 147 ms\n",
      "CPU times: user 4.99 ms, sys: 1.8 ms, total: 6.79 ms\n",
      "Wall time: 45.5 ms\n",
      "CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.18 ms\n",
      "Wall time: 53.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Calling `cache()` flips a flag on the RDD. \n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using `unpersist()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Will `unpersist` uncache the RDD immediately or does it wait for an action?</summary>\n",
    "A: It unpersists immediately.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and Persistence\n",
    "\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "- Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[190] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Will the RDD be stored on disk at this point?</summary>\n",
    "A: No. It will get stored after we call an action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence Levels\n",
    "\n",
    "Level | Meaning\n",
    "--- | ---\n",
    "`MEMORY_ONLY` | Same as `cache()`\n",
    "`MEMORY_AND_DISK` | Cache in memory then overflow to disk\n",
    "`MEMORY_AND_DISK_SER` | Like above; in cache keep objects serialized instead of live \n",
    "`DISK_ONLY` | Cache to disk not to memory\n",
    "\n",
    "### Notes\n",
    "\n",
    "- `MEMORY_AND_DISK_SER` is a good compromise between the levels. \n",
    "- Fast, but not too expensive.\n",
    "- Make sure you unpersist when you don't need the RDD any more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib\n",
    "\n",
    "- MLlib is Spark’s machine learning (ML) library.\n",
    "- Its goal is to make practical machine learning scalable and easy.\n",
    "- It consists of common learning algorithms, including:\n",
    "  - Classification/Regression\n",
    "    - Logistic Regression, Support vector machine (SVM), Naive Bayes, Gradient Boosted Trees, Random Forests, Multilayer Perceptron (e.g., a neural network), Generalized linear regression (GLM)\n",
    "  - Recommenders/Collaborative Filtering\n",
    "    - Non-negative matrix factorization (NMF)\n",
    "  - Decomposition\n",
    "    - Singular value decomposition (SVD), (Principal component analysis)\n",
    "  - Clustering\n",
    "    - K-Means, Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "\n",
    "- *\"s3:\" URLs break when Secret Key contains a slash, even if encoded* <https://issues.apache.org/jira/browse/HADOOP-3733>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
